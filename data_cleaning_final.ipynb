{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc66f1f7",
   "metadata": {},
   "source": [
    "# DATA CLEANING #\n",
    "This notebook shows the code and process of cleaning the dataset prior to EDA and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d60edd",
   "metadata": {},
   "source": [
    "## Step 1. Import Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "695e68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib \n",
    "matplotlib.rcParams[\"figure.figsize\"] = (20,10)\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5139d04",
   "metadata": {},
   "source": [
    "## Step 2. Load and Inspect Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99806cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and inspect the first 5 rows.\n",
    "df1 = pd.read_csv(\"../data/mas_housing 2.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53934571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataframe.\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column.\n",
    "df1.isnull().sum()\n",
    "\n",
    "# Observation: Many nulls in dataset. Car Parks and Furnishing have high nulls.\n",
    "# Imputation to be handled later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6851d",
   "metadata": {},
   "source": [
    "## Step 3.  Clean *Location* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine counts for each location and look for duplicates due to variations in naming.\n",
    "\n",
    "df1['Location'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc63312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant city names and normalize casing\n",
    "# For location only retain area name, remove KL and commas. Standardized KLCC.\n",
    "\n",
    "df1['Location'] = df1['Location'].str.replace(', Kuala Lumpur', '', regex=False)\n",
    "df1['Location'] = df1['Location'].str.replace(r'\\bklcc\\b', 'KLCC', case=False, regex=True)\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e31046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some locations are named 'Other' which should be removed. Determine the count of 'Other' locations.\n",
    "\n",
    "df1[df1['Location'] == 'Other'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Other' locations from the dataset.\n",
    "\n",
    "df1 = df1[df1['Location'] != 'Other']\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df1['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0dc6d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize capitalization for specific entries.\n",
    "\n",
    "df1['Location'] = df1['Location'].replace('SANTUARI PARK PANTAI', 'Santuari Park Pantai')\n",
    "df1['Location'] = df1['Location'].replace('cyberjaya', 'Cyberjaya')\n",
    "df1['Location'] = df1['Location'].replace('U-THANT', 'U-Thant')\n",
    "df1['Location'] = df1['Location'].replace('TAMAN MELATI', 'Taman Melati')\n",
    "df1['Location'] = df1['Location'].replace('SEMARAK', 'Semarak')\n",
    "df1['Location'] = df1['Location'].replace('Off Gasing Indah,', 'Gasing Indah')\n",
    "df1['Location'] = df1['Location'].replace('kepong', 'Kepong')\n",
    "df1['Location'] = df1['Location'].replace('ADIVA Desa ParkCity', 'Adiva Desa Park City')\n",
    "df1['Location'] = df1['Location'].replace('Sungai Long SL8', 'Sungai Long')\n",
    "df1['Location'] = df1['Location'].replace('taman connaught', 'Taman Connaught')\n",
    "df1['Location'] = df1['Location'].replace('duta Nusantara', 'Duta Nusantara')\n",
    "df1['Location'] = df1['Location'].replace('taman cheras perdana', 'Taman Cheras Perdana')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f6664324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-KL locations.\n",
    "\n",
    "df1 = df1[~df1['Location'].str.contains('Landed Sd', na=False)]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df1 = df1[~df1['Location'].str.contains('Singapore', na=False)]\n",
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c476b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize location names for consistency.\n",
    "\n",
    "df1['Location'] = df1['Location'].str.replace('Taman Yarl, UOG', 'Taman Yarl', regex=False)\n",
    "df1['Location'] = df1['Location'].str.replace('Taman Yarl OUG', 'Taman Yarl', regex=False)\n",
    "df1['Location'] = df1['Location'].str.replace('TamanYarl', 'Taman Yarl', regex=False)\n",
    "df1['Location'] = df1['Location'].str.replace('Bandar Sri damansara', 'Bandar Sri Damansara', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final counts of locations after cleaning.\n",
    "\n",
    "df1['Location'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show complete counts for each column\n",
    "print(\"### Location Counts:\")\n",
    "print(df1['Location'].value_counts().sort_index())\n",
    "print(f\"\\nTotal unique locations: {df1['Location'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove records where Location is 'City Centre'\n",
    "print(f\"Records before removing City Centre: {len(df1)}\")\n",
    "df1 = df1[~df1['Location'].str.contains('City Centre', case=False, na=False)]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(f\"Records after removing City Centre: {len(df1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4edaa3",
   "metadata": {},
   "source": [
    "## Step 4. Clean *Price* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'RM' and commas and convert to numeric type and show summary statistics\n",
    "\n",
    "df1['Price'] = (\n",
    "    df1['Price']\n",
    "    .astype(str)\n",
    "    .str.replace('RM', '', regex=False)\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .str.strip()\n",
    "    .replace({'': np.nan, '<NA>': np.nan})\n",
    "    .astype(float)\n",
    "    .astype('Int64')  # Nullable integer\n",
    ")\n",
    " \n",
    "print(df1['Price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records where Price is below 100,000 or above 10,000,000 and show summary statistics.\n",
    "\n",
    "df1 = df1[(df1['Price'] >= 100000) & (df1['Price'] <= 10000000)]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(df1['Price'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c6e00",
   "metadata": {},
   "source": [
    "## Step 5. Clean *Rooms* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b0894238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rooms column has mixed types, some are strings with '+' indicating multiple rooms. Sum the numbers in such cases, replace the output and convert to integer.\n",
    "\n",
    "def sum_rooms(val):\n",
    "    if isinstance(val, str) and '+' in val:\n",
    "        parts = re.findall(r'\\d+', val)\n",
    "        if len(parts) == 2:\n",
    "            return int(parts[0]) + int(parts[1])\n",
    "    try:\n",
    "        return int(val)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df1.loc[:, 'Rooms'] = df1['Rooms'].apply(sum_rooms)\n",
    "df1.loc[:, 'Rooms'] = df1['Rooms'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing value records and convert column to integer\n",
    "\n",
    "# Check number of records before cleaning\n",
    "print(f\"Records before removing missing values: {len(df1)}\")\n",
    "\n",
    "# Remove missing values\n",
    "df1 = df1.dropna(subset=['Rooms'])\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert to integer type\n",
    "df1['Rooms'] = df1['Rooms'].astype(int)\n",
    "\n",
    "# Check number of records after cleaning\n",
    "print(f\"Records after removing missing values: {len(df1)}\")\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nRooms column data type:\", df1['Rooms'].dtype)\n",
    "print(\"\\nUnique values in Rooms column:\")\n",
    "print(df1['Rooms'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records where Rooms is greater than 7 since they are outliers.\n",
    "\n",
    "df1 = df1[df1['Rooms'] <= 7]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(df1['Rooms'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157a324",
   "metadata": {},
   "source": [
    "## Step 6. Clean *Bathrooms* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics and value counts for Bathrooms.\n",
    "\n",
    "print(df1['Bathrooms'].describe())\n",
    "print(df1['Bathrooms'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records where Bathrooms is greater than 7 since they are outliers.\n",
    "\n",
    "df1 = df1[df1['Bathrooms'] <= 7]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(df1['Bathrooms'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7963b",
   "metadata": {},
   "source": [
    "## Step 7. Clean *Car Parks* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d70bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics and value counts for Car Parks\n",
    "\n",
    "print(df1['Car Parks'].describe())\n",
    "print(df1['Car Parks'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records where Car Parks is greater than 4 since they are outliers.\n",
    "\n",
    "df1 = df1[df1['Car Parks'] <= 4]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(df1['Car Parks'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19877f89",
   "metadata": {},
   "source": [
    "## Step 8. Clean *Property Type* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique property types and their count.\n",
    "\n",
    "print(df1['Property Type'].unique())\n",
    "print(f\"\\nTotal unique property types: {df1['Property Type'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all records with 'Residential Land'.\n",
    "\n",
    "df1 = df1[~df1['Property Type'].str.startswith('Residential Land', na=False)]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(f\"\\nTotal unique property types: {df1['Property Type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse values into multiple columns of Type, Storeys, Position, Layout, Land Status\n",
    "\n",
    "# Function to parse property strings\n",
    "def parse_property(text):\n",
    "    # Extract storeys if any (e.g., 2-sty, 3.5-sty)\n",
    "    storey_match = re.search(r'(\\d+(\\.\\d+)?)-sty', text)\n",
    "    storeys = float(storey_match.group(1)) if storey_match else None\n",
    "\n",
    "    # Extract bracket content (e.g., Corner, EndLot, Penthouse)\n",
    "    bracket_match = re.search(r'\\((.*?)\\)', text)\n",
    "    detail = bracket_match.group(1).strip() if bracket_match else None\n",
    "\n",
    "    # Remove storey and bracketed details from base type\n",
    "    base = re.sub(r'(\\d+(\\.\\d+)?)-sty\\s*', '', text)  # remove \"X-sty\"\n",
    "    base = re.sub(r'\\s*\\(.*\\)', '', base)            # remove \"(...)\"\n",
    "    base = base.strip()\n",
    "\n",
    "    # Identify categories\n",
    "    positions = ['Corner', 'EndLot', 'Intermediate']\n",
    "    layouts = ['Penthouse', 'Duplex', 'Triplex', 'Studio', 'SOHO']\n",
    "    land_status = 'Land' if 'Land' in base else None\n",
    "    position = detail if detail in positions else None\n",
    "    layout = detail if detail in layouts else None\n",
    "\n",
    "    return pd.Series([base, storeys, position, layout, land_status])\n",
    "\n",
    "# Apply parsing to df1\n",
    "\n",
    "df1[['MainType', 'Storeys', 'Position', 'Layout', 'LandStatus']] = df1['Property Type'].apply(parse_property)\n",
    "\n",
    "# Standardise MainType (merge similar terms)\n",
    "\n",
    "df1['MainType'] = df1['MainType'].replace({\n",
    "    'Terrace/Link House': 'Terrace/Link House',\n",
    "    'Cluster House': 'Cluster House',\n",
    "    'Bungalow Land': 'Bungalow Land',\n",
    "    'Bungalow': 'Bungalow',\n",
    "    'Semi-detached House': 'Semi-detached House',\n",
    "    'Condominium': 'Condominium',\n",
    "    'Serviced Residence': 'Serviced Residence',\n",
    "    'Apartment': 'Apartment',\n",
    "    'Flat': 'Flat',\n",
    "    'Townhouse': 'Townhouse'\n",
    "})\n",
    "\n",
    "# Preview\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab3f95",
   "metadata": {},
   "source": [
    "## Step 9. Clean *Size* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e14480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records with missing values in 'Size' and reset index\n",
    "\n",
    "print(f\"Records before removing missing Size: {len(df1)}\")\n",
    "df1 = df1.dropna(subset=['Size'])\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(f\"Records after removing missing Size: {len(df1)}\")\n",
    "\n",
    "# Quick verification\n",
    "print(\"Missing Size count:\", df1['Size'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77707d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Built-up', 'Land area' (case-insensitive) and colons from Size column, trim whitespace\n",
    "\n",
    "df1['Size'] = df1['Size'].astype(str)\n",
    "df1['Size'] = df1['Size'].str.replace(r'(?i)\\b(?:built-?up|land area)\\b', '', regex=True)  # remove words\n",
    "df1['Size'] = df1['Size'].str.replace(':', '', regex=False)                                    # remove colons\n",
    "df1['Size'] = df1['Size'].str.replace(r'[\\u00A0\\s]+', ' ', regex=True).str.strip()             # normalize whitespace\n",
    "df1.loc[df1['Size'].isin(['', 'nan', 'None']), 'Size'] = np.nan                                # empty -> NaN\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample cleaned Size values:\")\n",
    "print(df1['Size'].dropna().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee640480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List distinct Size values that contain letters (text-only entries)\n",
    "\n",
    "s = df1['Size'].dropna().astype(str).str.strip()\n",
    "text_mask = s.str.contains(r'[A-Za-z]', regex=True)\n",
    "unique_text_sizes = pd.Series(s[text_mask].unique()).sort_values()\n",
    "\n",
    "print(f\"Distinct text-only Size values ({len(unique_text_sizes)}):\")\n",
    "for v in unique_text_sizes:\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove apostrophes and bracketed content from Size column.\n",
    "\n",
    "df1['Size'] = df1['Size'].astype(str)\n",
    "df1['Size'] = df1['Size'].str.replace(\"'\", \"\", regex=False)  # remove apostrophes\n",
    "df1['Size'] = df1['Size'].str.replace(r'\\([^)]*\\)', '', regex=True)  # remove bracketed content\n",
    "df1['Size'] = df1['Size'].str.strip()  # remove extra whitespace\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample cleaned Size values:\")\n",
    "print(df1['Size'].dropna().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate multiplication for entries like '20 x 30' or '15X25' in Size column.\n",
    "\n",
    "# Function to multiply numbers connected by x/X\n",
    "\n",
    "def multiply_dimensions(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    # Convert to string and lowercase\n",
    "    s = str(val).lower()\n",
    "    # Look for pattern: number x number\n",
    "    match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*[xX]\\s*(\\d+(?:\\.\\d+)?)', s)\n",
    "    if match:\n",
    "        try:\n",
    "            num1 = float(match.group(1))\n",
    "            num2 = float(match.group(2))\n",
    "            return num1 * num2\n",
    "        except ValueError:\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "# Apply the multiplication.\n",
    "\n",
    "df1['Size'] = df1['Size'].apply(multiply_dimensions)\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample multiplied Size values:\")\n",
    "print(df1['Size'].dropna().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc92951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select entries with ranges like '800-900' and clean them.\n",
    "\n",
    "# Function to handle ranges (keep first number before dash)\n",
    "\n",
    "def clean_ranges(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    # Convert to string\n",
    "    s = str(val)\n",
    "    # Look for pattern: number-number\n",
    "    match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*\\d+(?:\\.\\d+)?', s)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))  # Return only the first number\n",
    "        except ValueError:\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "# Apply the range cleaning.\n",
    "\n",
    "df1['Size'] = df1['Size'].apply(clean_ranges)\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample cleaned range values:\")\n",
    "print(df1['Size'].dropna().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5aad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas from Size column.\n",
    "\n",
    "df1['Size'] = df1['Size'].astype(str).str.replace(',', '', regex=False)\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample values after removing commas:\")\n",
    "print(df1['Size'].dropna().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'sq. ft.' from Size column.\n",
    "\n",
    "df1['Size'] = df1['Size'].astype(str).str.replace('sq. ft.', '', regex=False)\n",
    "df1['Size'] = df1['Size'].str.strip()  # remove any trailing whitespace\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample values after removing 'sq. ft.':\")\n",
    "print(df1['Size'].dropna().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a77ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all text from Size column.\n",
    " \n",
    "df1['Size'] = df1['Size'].astype(str)\n",
    "df1['Size'] = df1['Size'].str.replace(r'[A-Za-z]+', '', regex=True)  # remove all letters\n",
    "df1['Size'] = df1['Size'].str.strip()  # remove trailing whitespace\n",
    "\n",
    "# Convert to float and handle any remaining non-numeric values.\n",
    "\n",
    "df1['Size'] = pd.to_numeric(df1['Size'], errors='coerce')\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample values after removing all text:\")\n",
    "print(df1['Size'].dropna().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16397f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove decimals by converting to integers.\n",
    "\n",
    "df1['Size'] = df1['Size'].astype(float).round().astype('Int64')\n",
    "\n",
    "# Quick check.\n",
    "\n",
    "print(\"Sample values after removing decimals:\")\n",
    "print(df1['Size'].head(20))\n",
    "print(\"\\nColumn type:\", df1['Size'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ffb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records with Size values outside reasonable range.\n",
    "\n",
    "print(f\"Records before filtering Size range: {len(df1)}\")\n",
    "\n",
    "df1 = df1[(df1['Size'] >= 400) & (df1['Size'] <= 10000)]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Records after filtering Size range: {len(df1)}\")\n",
    "\n",
    "# Verify the changes.\n",
    "\n",
    "print(\"\\nSize statistics after filtering:\")\n",
    "print(df1['Size'].describe())\n",
    "print(\"\\nSmallest sizes:\", df1['Size'].nsmallest(5))\n",
    "print(\"Largest sizes:\", df1['Size'].nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining values containing special symbols.\n",
    "\n",
    "special_chars = r'[#xX*\\+\\-\\\\/\\(\\)\\[\\]\\{\\}\\^\\$\\&\\%\\@\\!\\?\\,\\.]'\n",
    "symbol_mask = df1['Size'].astype(str).str.contains(special_chars, regex=True)\n",
    "special_values = pd.Series(df1.loc[symbol_mask, 'Size'].unique()).sort_values()\n",
    "\n",
    "print(f\"Values containing special symbols ({len(special_values)}):\")\n",
    "for v in special_values:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5086ff9",
   "metadata": {},
   "source": [
    "## Step 10. Clean *Furnishing* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records with missing values in Furnishing column.\n",
    "\n",
    "print(f\"Records before removing missing Furnishing: {len(df1)}\")\n",
    "df1 = df1.dropna(subset=['Furnishing'])\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "print(f\"Records after removing missing Furnishing: {len(df1)}\")\n",
    "\n",
    "# Quick verification.\n",
    "\n",
    "print(\"\\nUnique Furnishing values:\")\n",
    "print(df1['Furnishing'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records with missing values and 'Unknown' in Furnishing column.\n",
    "\n",
    "print(f\"Records before cleaning Furnishing: {len(df1)}\")\n",
    "\n",
    "# Remove missing values and 'Unknown'.\n",
    "\n",
    "df1 = df1.dropna(subset=['Furnishing'])\n",
    "df1 = df1[~df1['Furnishing'].str.contains('Unknown', case=False, na=False)]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Records after cleaning Furnishing: {len(df1)}\")\n",
    "\n",
    "# Quick verification.\n",
    "\n",
    "print(\"\\nUnique Furnishing values:\")\n",
    "print(df1['Furnishing'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416ea9a",
   "metadata": {},
   "source": [
    "## Step 11. Rearrange columns of the data frame and remove *Layout* and *Land Status* columns since there are too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure MainType exists (create if missing), then replace Property Type with MainType and rename to 'Type'.\n",
    "\n",
    "if 'MainType' not in df1.columns:\n",
    "    # parse_property is defined earlier in this notebook; reuse it to create MainType\n",
    "    df1[['MainType', 'Storeys', 'Position', 'Layout', 'LandStatus']] = df1['Property Type'].apply(parse_property)\n",
    "\n",
    "# Create new column 'Type' from MainType, drop original columns.\n",
    "\n",
    "df1['Type'] = df1['MainType']\n",
    "if 'Property Type' in df1.columns:\n",
    "    df1.drop(columns=['Property Type'], inplace=True)\n",
    "if 'MainType' in df1.columns:\n",
    "    df1.drop(columns=['MainType'], inplace=True)\n",
    "\n",
    "# Quick verification.\n",
    "\n",
    "print(\"Columns now include 'Type':\", 'Type' in df1.columns)\n",
    "print(\"Sample Type values:\", pd.Series(df1['Type'].unique()).sort_values()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfc7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values of 'Type' column (sorted) and show counts.\n",
    "\n",
    "print(f\"Unique Type count: {df1['Type'].nunique()}\\n\")\n",
    "print(\"Sorted unique values:\")\n",
    "for v in sorted(df1['Type'].dropna().unique()):\n",
    "    print(v)\n",
    "\n",
    "print(\"\\nValue counts:\")\n",
    "print(df1['Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4de7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Bungalow Land' and 'Cluster House' from Type (case-insensitive).\n",
    "\n",
    "print(f\"Records before removing Types: {len(df1)}\")\n",
    "mask_remove = df1['Type'].astype(str).str.contains(r'^(?:Bungalow Land|Cluster House)$', case=False, na=False)\n",
    "removed = mask_remove.sum()\n",
    "df1 = df1[~mask_remove].reset_index(drop=True)\n",
    "print(f\"Removed {removed} records. Records after removal: {len(df1)}\")\n",
    "\n",
    "# Quick verification.\n",
    "\n",
    "print(f\"\\nUnique Type count after removal: {df1['Type'].nunique()}\")\n",
    "print(df1['Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06434c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'Type' column to immediately after 'Location' (if both exist).\n",
    "\n",
    "if 'Type' in df1.columns and 'Location' in df1.columns:\n",
    "    cols = df1.columns.tolist()\n",
    "    cols.remove('Type')\n",
    "    lon_idx = cols.index('Location')\n",
    "    cols.insert(lon_idx + 1, 'Type')\n",
    "    df1 = df1[cols]\n",
    "    print(\"Moved 'Type' after 'Location'.\")\n",
    "else:\n",
    "    print(\"No change: 'Type' or 'Location' column not found.\")\n",
    "    \n",
    "# Quick check.\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Layout' and 'LandStatus' if they exist, then move 'Price' to the end.\n",
    "\n",
    "for col in ['Layout', 'LandStatus']:\n",
    "    if col in df1.columns:\n",
    "        df1.drop(columns=[col], inplace=True)\n",
    "        print(f\"Dropped column: {col}\")\n",
    "\n",
    "if 'Price' in df1.columns:\n",
    "    cols = [c for c in df1.columns if c != 'Price'] + ['Price']\n",
    "    df1 = df1[cols]\n",
    "    print(\"Moved 'Price' to the end of the dataframe.\")\n",
    "else:\n",
    "    print(\"Column 'Price' not found â€” no reordering performed.\")\n",
    "\n",
    "# Quick verification.\n",
    "\n",
    "print(df1.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337b6a6",
   "metadata": {},
   "source": [
    "## Step 12. Clean *Storeys* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique Type values with sum of missing Storeys beside each.\n",
    "\n",
    "missing_storeys = (\n",
    "    df1.groupby(df1['Type'].fillna('<MISSING TYPE>'))['Storeys']\n",
    "       .apply(lambda s: s.isna().sum())\n",
    "       .reset_index(name='Missing_Storeys')\n",
    "       .sort_values('Missing_Storeys', ascending=False)\n",
    ")\n",
    "print(missing_storeys.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustly clean Storeys, fill missing with 1, then convert to numpy int64.\n",
    "\n",
    "print(f\"Missing Storeys before clean: {df1['Storeys'].isna().sum()}\")\n",
    "\n",
    "# Extract leading numeric (handles '3-sty', '3.5-sty', '3', etc.).\n",
    "\n",
    "df1['Storeys'] = df1['Storeys'].astype(str).str.extract(r'(\\d+(?:\\.\\d+)?)', expand=False)\n",
    "\n",
    "# Convert to numeric (coerce non-numeric to NaN).\n",
    "\n",
    "df1['Storeys'] = pd.to_numeric(df1['Storeys'], errors='coerce')\n",
    "\n",
    "# Fill NaNs with 1, round any floats and convert to int64.\n",
    "\n",
    "df1['Storeys'] = df1['Storeys'].fillna(1).round().astype('int64')\n",
    "\n",
    "print(f\"Missing Storeys after fill: {df1['Storeys'].isna().sum()}\")\n",
    "print(f\"Storeys dtype: {df1['Storeys'].dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881e1b5",
   "metadata": {},
   "source": [
    "## Step 13. Clean *Position* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9891da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique Position values per Type and sum their occurrences.\n",
    "\n",
    "tmp = df1.copy()\n",
    "tmp['Position'] = tmp['Position'].fillna('<MISSING>')\n",
    "\n",
    "# Counts per Type x Position.\n",
    "\n",
    "pos_counts = (\n",
    "    tmp.groupby(['Type', 'Position'])\n",
    "       .size()\n",
    "       .reset_index(name='Count')\n",
    "       .sort_values(['Type', 'Count'], ascending=[True, False])\n",
    ")\n",
    "print(\"Counts by Type and Position:\")\n",
    "print(pos_counts.to_string(index=False))\n",
    "\n",
    "# Summary per Type: unique positions (comma-separated), number of unique positions, and total occurrences.\n",
    "\n",
    "summary = (\n",
    "    pos_counts.groupby('Type')\n",
    "              .agg(UniquePositions=('Position', lambda s: ', '.join(sorted(s.unique()))),\n",
    "                   NumUnique=('Position', 'nunique'),\n",
    "                   TotalCount=('Count', 'sum'))\n",
    "              .reset_index()\n",
    "              .sort_values('TotalCount', ascending=False)\n",
    ")\n",
    "print(\"\\nSummary per Type:\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a993c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing Position values with 'Unknown' and normalize empty strings.\n",
    "\n",
    "print(f\"Position missing before: {df1['Position'].isna().sum()}\")\n",
    "df1['Position'] = df1['Position'].fillna('Unknown')\n",
    "df1['Position'] = df1['Position'].replace(r'^\\s*$', 'Unknown', regex=True)  # catch empty/blank strings\n",
    "df1['Position'] = df1['Position'].astype(str)\n",
    "print(f\"Position missing after: {df1['Position'].isna().sum()}\")\n",
    "print(df1['Position'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique Position values and their counts.\n",
    "\n",
    "positions = df1['Position'].fillna('Unknown').astype(str).str.strip()\n",
    "print(\"Unique Position values (sorted):\")\n",
    "print(sorted(positions.unique()))\n",
    "print(\"\\nPosition value counts (sorted by name):\")\n",
    "print(positions.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af73010",
   "metadata": {},
   "source": [
    "## Step 14. Save cleaned data into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save cleaned dataframe to CSV\n",
    "df1.to_csv(\"../data/cleaned_data_final.csv\", index=False)\n",
    "print(\"Saved cleaned_data to ../data/cleaned_data_final.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
